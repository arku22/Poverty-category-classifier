{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set a few plotting defaults\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.rcParams['patch.edgecolor'] = 'k'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9c541a166077ce34eadcde7a702b4e1f0d3bbba6"
   },
   "source": [
    "### Read in Data and Look at Summary Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_columns = 150\n",
    "\n",
    "# Read in data\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b180722a8e9a1688463f0340e4830e37a5586c4a"
   },
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "61687f25325218b77d4d75804044c7083da85eb7"
   },
   "outputs": [],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3c0d0640f3867d2c290ac58a55639fefb08875c0"
   },
   "source": [
    "#### Integer Columns\n",
    "\n",
    "Let's look at the distribution of unique values in the integer columns. For each column, we'll count the number of unique values and show the result in a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7db394756abedd215885bc7d6b9dc2c767964b35"
   },
   "outputs": [],
   "source": [
    "train.select_dtypes(np.int64).nunique().value_counts().sort_index().plot.bar(color = 'blue', \n",
    "                                                                             figsize = (8, 6),\n",
    "                                                                            edgecolor = 'k', linewidth = 2);\n",
    "plt.xlabel('Number of Unique Values'); plt.ylabel('Count');\n",
    "plt.title('Count of Unique Values in Integer Columns');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1a9db9a6caa3b77b6a6f0184644c1e11b2ed53d5"
   },
   "source": [
    "The columns with only 2 unique values represent Booleans (0 or 1). In a lot of cases, this boolean information is already on a household level. For example, the `refrig` column says whether or not the household has a refrigerator. When it comes time to make features from the Boolean columns that are on the household level, we will _not need to aggregate_ these. However, the Boolean columns that are on the individual level will need to be aggregated. \n",
    "\n",
    "#### Float Columns\n",
    "\n",
    "Another column type is floats which represent continuous variables. We can make a quick distribution plot to show the distribution of all float columns. We'll use an [`OrderedDict`](https://pymotw.com/2/collections/ordereddict.html) to map the poverty levels to colors because this keeps the keys and values in the same order as we specify (unlike a regular Python dictionary).\n",
    "\n",
    "The following graphs shows the distributions of the `float` columns colored by the value of the `Target`. With these plots, we can see if there is a significant difference in the variable distribution depending on the household poverty level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c22a1876cf1d8a717b0baab8564acb4a7f531b51",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "plt.figure(figsize = (20, 16))\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Color mapping\n",
    "colors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})\n",
    "poverty_mapping = OrderedDict({1: 'extreme', 2: 'moderate', 3: 'vulnerable', 4: 'non vulnerable'})\n",
    "\n",
    "# Iterate through the float columns\n",
    "for i, col in enumerate(train.select_dtypes('float')):\n",
    "    ax = plt.subplot(4, 2, i + 1)\n",
    "    # Iterate through the poverty levels\n",
    "    for poverty_level, color in colors.items():\n",
    "        # Plot each poverty level as a separate line\n",
    "        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(), \n",
    "                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n",
    "        \n",
    "    plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n",
    "\n",
    "plt.subplots_adjust(top = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "79ec245f541336c564fdce3d013b0b988627b6e5"
   },
   "source": [
    "Later on we'll calculate correlations between the variables and the `Target` to gauge the relationships between the features, but these plots can already give us a sense of which variables may be most \"relevant\" to a model. For example, the `meaneduc`, representing the average education of the adults in the household appears to be related to the poverty level: __a higher average adult education leads to higher values of the target which are less severe levels of poverty__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "355fd9746ec30af6661cf97a1243790730a5df99"
   },
   "source": [
    "#### Object Columns\n",
    "\n",
    "The last column type is `object` which we can view as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9b110b76084ba14045c4c69ce909148d91ce0ecc"
   },
   "outputs": [],
   "source": [
    "train.select_dtypes('object').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d1e660b7e06b81b61bec81a82be4a5854115d09a"
   },
   "source": [
    "The `Id` and `idhogar` object types make sense because these are identifying variables. However, the other columns seem to be a mix of strings and numbers which we'll need to address before doing any machine learning. According to the documentation for these columns:\n",
    "\n",
    "* `dependency`: Dependency rate, calculated = (number of members of the household younger than 19 or older than 64)/(number of member of household between 19 and 64)\n",
    "* `edjefe`: years of education of male head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n",
    "* `edjefa`: years of education of female head of household, based on the interaction of escolari (years of education), head of household and gender, yes=1 and no=0\n",
    "\n",
    "These explanations clear up the issue. For these three variables, __\"yes\" = 1__ and __\"no\" = 0__. We can correct the variables using a mapping and convert to floats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "acf4a663dd5d6713dad025ef6674b547cc02690f"
   },
   "outputs": [],
   "source": [
    "mapping = {\"yes\": 1, \"no\": 0}\n",
    "\n",
    "# Apply same operation to both train and test\n",
    "for df in [train, test]:\n",
    "    # Fill in the values with the correct mapping\n",
    "    df['dependency'] = df['dependency'].replace(mapping).astype(np.float64)\n",
    "    df['edjefa'] = df['edjefa'].replace(mapping).astype(np.float64)\n",
    "    df['edjefe'] = df['edjefe'].replace(mapping).astype(np.float64)\n",
    "\n",
    "train[['dependency', 'edjefa', 'edjefe']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "55af5988853fd538f674b07f93d5655c133ed744"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (16, 12))\n",
    "\n",
    "# Iterate through the float columns\n",
    "for i, col in enumerate(['dependency', 'edjefa', 'edjefe']):\n",
    "    ax = plt.subplot(3, 1, i + 1)\n",
    "    # Iterate through the poverty levels\n",
    "    for poverty_level, color in colors.items():\n",
    "        # Plot each poverty level as a separate line\n",
    "        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(), \n",
    "                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n",
    "        \n",
    "    plt.title(f'{col.capitalize()} Distribution'); plt.xlabel(f'{col}'); plt.ylabel('Density')\n",
    "\n",
    "plt.subplots_adjust(top = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e2fb233b0d5876e71ba33e6b5a8d29e2429fab94"
   },
   "source": [
    "These variables are now correctly represented as numbers and can be fed into a machine learning model. \n",
    "\n",
    "To make operations like that above a little easier, we'll join together the training and testing dataframes. This is important once we start feature engineering because we want to apply the same operations to both dataframes so we end up with the same features. Later we can separate out the sets based on the `Target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "59a54206932e6414745226a101097b69b7845107"
   },
   "outputs": [],
   "source": [
    "# Add null Target column to test\n",
    "test['Target'] = np.nan\n",
    "data = train.append(test, ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "423ee29cdbb0975eb9d8f6f729e1595f401acbc3"
   },
   "source": [
    "## Exploring Label Distribution\n",
    "\n",
    "Next, we can get an idea of how imbalanced the problem is by looking at the distribution of labels. There are four possible integer levels, indicating four different levels of poverty. To look at the correct labels, we'll subset only to the columns where `parentesco1 == 1` because this is the head of household, the correct label for each household.\n",
    "\n",
    "The bar plot below shows the distribution of training labels (since there are no testing labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6ed7b957df6b758a5bf586c682289edcb1a78f66"
   },
   "outputs": [],
   "source": [
    "# Heads of household\n",
    "heads = data.loc[data['parentesco1'] == 1].copy()\n",
    "\n",
    "# Labels for training\n",
    "train_labels = data.loc[(data['Target'].notnull()) & (data['parentesco1'] == 1), ['Target', 'idhogar']]\n",
    "\n",
    "# Value counts of target\n",
    "label_counts = train_labels['Target'].value_counts().sort_index()\n",
    "\n",
    "# Bar plot of occurrences of each label\n",
    "label_counts.plot.bar(figsize = (8, 6), \n",
    "                      color = colors.values(),\n",
    "                      edgecolor = 'k', linewidth = 2)\n",
    "\n",
    "# Formatting\n",
    "plt.xlabel('Poverty Level'); plt.ylabel('Count'); \n",
    "plt.xticks([x - 1 for x in poverty_mapping.keys()], \n",
    "           list(poverty_mapping.values()), rotation = 60)\n",
    "plt.title('Poverty Level Breakdown');\n",
    "\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4b4e7cebd091b34469e78c123eac55410dfb8e77"
   },
   "source": [
    "\n",
    "### Identify Errors\n",
    "\n",
    "First we need to find the errors before we can correct them. To find the households with different labels for  family members, we can group the data by the household and then check if there is only one unique value of the `Target`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "81b7d45bfe8fd0feb09c307c5f2c9f00a6f8bc73"
   },
   "outputs": [],
   "source": [
    "# Groupby the household and figure out the number of unique values\n",
    "all_equal = train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n",
    "\n",
    "# Households where targets are not all equal\n",
    "not_equal = all_equal[all_equal != True]\n",
    "print('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "25af32cb8437379e65c2100e4b1d2f26ef5f0410"
   },
   "source": [
    "Let's look at one example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c3c17e4f9a62990e08d9704d51917b0ace16d54f"
   },
   "outputs": [],
   "source": [
    "train[train['idhogar'] == not_equal.index[0]][['idhogar', 'parentesco1', 'Target']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5f3bfc8ada58df621ab0d9bb789a161e9db834b9"
   },
   "source": [
    "### Families without Heads of Household\n",
    "\n",
    "We can correct all the label discrepancies by assigning the individuals in the same household the label of the head of household. But wait, you may ask: \"What if there are households without a head of household? And what if the members of those households have differing values of the label?\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "32c355c2537617ddb4806d4b2b1300817d5b7d99"
   },
   "outputs": [],
   "source": [
    "households_leader = train.groupby('idhogar')['parentesco1'].sum()\n",
    "\n",
    "# Find households without a head\n",
    "households_no_head = train.loc[train['idhogar'].isin(households_leader[households_leader == 0].index), :]\n",
    "\n",
    "print('There are {} households without a head.'.format(households_no_head['idhogar'].nunique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "317ab2763756312d66fcbe92d8d320878e560c1b"
   },
   "outputs": [],
   "source": [
    "# Find households without a head and where labels are different\n",
    "households_no_head_equal = households_no_head.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n",
    "print('{} Households with no head have different labels.'.format(sum(households_no_head_equal == False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "88929859b4ba55bed14c3ec95f0bc7e60aa5c5c7"
   },
   "source": [
    "Well that's a relief! This means that we don't have to worry about a household both where there is no head __AND__ the members have different values of the label! For this problem__if a household does not have a head, then there is no true label. Therefore, we actually won't use any of the households without a head for training__\n",
    "\n",
    "### Correct Errors\n",
    "\n",
    "Now we can correct labels for the households that do have a head __AND__ the members have different poverty levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8b3a4200ee6425d54d6321503fdd14bc7c8f2756"
   },
   "outputs": [],
   "source": [
    "# Iterate through each household\n",
    "for household in not_equal.index:\n",
    "    # Find the correct label (for the head of household)\n",
    "    true_target = int(train[(train['idhogar'] == household) & (train['parentesco1'] == 1.0)]['Target'])\n",
    "    \n",
    "    # Set the correct label for all members in the household\n",
    "    train.loc[train['idhogar'] == household, 'Target'] = true_target\n",
    "    \n",
    "    \n",
    "# Groupby the household and figure out the number of unique values\n",
    "all_equal = train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n",
    "\n",
    "# Households where targets are not all equal\n",
    "not_equal = all_equal[all_equal != True]\n",
    "print('There are {} households where the family members do not all have the same target.'.format(len(not_equal)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d1eb1771bc629d6f02452dc4e3edf669bffd1b3d"
   },
   "source": [
    "Since we are only going to use the heads of household for the labels, __this step is not completely necessary but it shows a workflow for correcting data errors like you may encounter in real life__. Don't consider it extra work, just practice for your career! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "47dd23a8222bd1d89512662dcb299b2ee6b6b04e"
   },
   "outputs": [],
   "source": [
    "# Number of missing in each column\n",
    "missing = pd.DataFrame(data.isnull().sum()).rename(columns = {0: 'total'})\n",
    "\n",
    "# Create a percentage missing\n",
    "missing['percent'] = missing['total'] / len(data)\n",
    "\n",
    "missing.sort_values('percent', ascending = False).head(10).drop('Target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7134602d0d3cd3045fb97eaaa724592b2d23553a"
   },
   "source": [
    "We don't have to worry about the `Target` becuase we made that `NaN` for the test data. However, we do need to address the other 3 columns with a high percentage of missing values.\n",
    "\n",
    "__v18q1__: Number of tablets\n",
    "\n",
    "Let's start with `v18q1` which indicates the number of tablets owned by a family. We can look at the value counts of this variable. Since this is a household variable, it only makes sense to look at it on a household level, so we'll only select the rows for the head of household.\n",
    "\n",
    "#### Function to Plot Value Counts\n",
    "\n",
    "Since we might want to plot value counts for different columns, we can write a simple function that will do it for us!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1eadbc5c6266518b2f0a047449966662ef8ea538"
   },
   "outputs": [],
   "source": [
    "def plot_value_counts(df, col, heads_only = False):\n",
    "    \"\"\"Plot value counts of a column, optionally with only the heads of a household\"\"\"\n",
    "    # Select heads of household\n",
    "    if heads_only:\n",
    "        df = df.loc[df['parentesco1'] == 1].copy()\n",
    "        \n",
    "    plt.figure(figsize = (8, 6))\n",
    "    df[col].value_counts().sort_index().plot.bar(color = 'blue',\n",
    "                                                 edgecolor = 'k',\n",
    "                                                 linewidth = 2)\n",
    "    plt.xlabel(f'{col}'); plt.title(f'{col} Value Counts'); plt.ylabel('Count')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7d5df5fccf64cd3ccecbb1c1bf2c2133a77c31e5"
   },
   "outputs": [],
   "source": [
    "plot_value_counts(heads, 'v18q1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cd53530fcdb192d6f7b6512c33ca134cf7984bed"
   },
   "source": [
    "It looks like the most common number of tablets to own is 1 if we go only by the data that is present. However, we also need to think about the data that is missing. In this case, it could be that families with a `nan` in this category just do not own a tablet! If we look at the data definitions, we see that `v18q` indicates whether or not a family owns a tablet. We should investigate this column combined with the number of tablets to see if our hypothesis holds.\n",
    "\n",
    "We can `groupby` the value of `v18q` (which is 1 for owns a tablet and 0 for does not) and then calculate the number of null values for `v18q1`. This will tell us if the null values represent that the family does not own a tablet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "19c44193187c0db95649a60447b6e0fe43703795"
   },
   "outputs": [],
   "source": [
    "heads.groupby('v18q')['v18q1'].apply(lambda x: x.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "adff3153e886d7adc1ced34c157a747fd0c4e9a1"
   },
   "source": [
    "Well, that solves the issue! Every family that has `nan` for `v18q1` does not own a tablet. Therefore, we can fill in this missing value with zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "20291bd2766b006bca3255034c4d1432e8024b0d"
   },
   "outputs": [],
   "source": [
    "data['v18q1'] = data['v18q1'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "42662b00e496efdd18cf3fc9cf497604cf873896"
   },
   "source": [
    "__v2a1__: Monthly rent payment\n",
    "\n",
    "The next missing column is `v2a1` which represents the montly rent payment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a425dc1a313ce8b75f64d4aa034513b84694c6ba"
   },
   "source": [
    "In addition to looking at the missing values of the monthly rent payment, it will be interesting to also look at the distribution of `tipovivi_`, the columns showing the ownership/renting status of the home. For this plot, we show the ownership status of those homes with a `nan` for the monthyl rent payment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "136b2e5c08622f91d56a275d278e9a4731a7396f"
   },
   "outputs": [],
   "source": [
    "# Variables indicating home ownership\n",
    "own_variables = [x for x in data if x.startswith('tipo')]\n",
    "\n",
    "\n",
    "# Plot of the home ownership variables for home missing rent payments\n",
    "data.loc[data['v2a1'].isnull(), own_variables].sum().plot.bar(figsize = (10, 8),\n",
    "                                                                        color = 'green',\n",
    "                                                              edgecolor = 'k', linewidth = 2);\n",
    "plt.xticks([0, 1, 2, 3, 4],\n",
    "           ['Owns and Paid Off', 'Owns and Paying', 'Rented', 'Precarious', 'Other'],\n",
    "          rotation = 60)\n",
    "plt.title('Home Ownership Status for Households Missing Rent Payments', size = 18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d072b3df8b6486cb90c1e9832094c7e13680a856"
   },
   "source": [
    "The meaning of the home ownership variables is below:\n",
    "\n",
    "    tipovivi1, =1 own and fully paid house\n",
    "    tipovivi2, \"=1 own,  paying in installments\"\n",
    "    tipovivi3, =1 rented\n",
    "    tipovivi4, =1 precarious\n",
    "    tipovivi5, \"=1 other(assigned,  borrowed)\"\n",
    "    \n",
    "We've solved the issue! Well, mostly: the households that do not have a monthly rent payment generally own their own home. In a few other situations, we are not sure of the reason for the missing information. \n",
    "\n",
    "For the houses that are owned and have a missing monthly rent payment, we can set the value of the rent payment to zero. For the other homes, we can leave the missing values to be imputed but we'll add a flag (Boolean) column indicating that these households had missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6ecfd2a5e15efa4ffe5436f43e84dbb5ea929314"
   },
   "outputs": [],
   "source": [
    "# Fill in households that own the house with 0 rent payment\n",
    "data.loc[(data['tipovivi1'] == 1), 'v2a1'] = 0\n",
    "\n",
    "# Create missing rent payment column\n",
    "data['v2a1-missing'] = data['v2a1'].isnull()\n",
    "\n",
    "data['v2a1-missing'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2e147ff8825964c53c634b05ad6653ebf8390539"
   },
   "source": [
    "__rez_esc__: years behind in school\n",
    "\n",
    "The last column with a high percentage of missing values is `rez_esc` indicating years behind in school. For the families with a null value, is possible that they have no children currently in school. Let's test this out by finding the ages of those who have a missing value in this column and the ages of those who do not have a missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d9576b2de07fbab360435f26d3d869f4f3b8a269"
   },
   "outputs": [],
   "source": [
    "data.loc[data['rez_esc'].notnull()]['age'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b7f67c2d888db26770ff3c0e108d003b34137d4a"
   },
   "source": [
    "What this tells us is that the oldest age with a missing value is 17. For anyone older than this, maybe we can assume that they are simply not in school. Let's look at the ages of those who have a missing value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b8b91108b4f7029b0642dd68052cc4b6d5906316"
   },
   "outputs": [],
   "source": [
    "data.loc[data['rez_esc'].isnull()]['age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "791b94a06ade9896fd5a7d6dc38152f831642a6a"
   },
   "outputs": [],
   "source": [
    "# If individual is over 19 or younger than 7 and missing years behind, set it to 0\n",
    "data.loc[((data['age'] > 19) | (data['age'] < 7)) & (data['rez_esc'].isnull()), 'rez_esc'] = 0\n",
    "\n",
    "# Add a flag for those between 7 and 19 with a missing value\n",
    "data['rez_esc-missing'] = data['rez_esc'].isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bce88b0ebef5a8d3eeb3ca4f978a0d4a69b1e185"
   },
   "source": [
    "There is also one outlier in the `rez_esc` column. Again, if we read through the competition discussions, we learn that the maximum value for this variable is 5. Therefore, any values above 5 should be set to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "18a972cc48d9e4818777afb7b06eab7e1f97855a"
   },
   "outputs": [],
   "source": [
    "data.loc[data['rez_esc'] > 5, 'rez_esc'] = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b3163f5a385c3f7d8fb7e3d38ea93526bf73e788"
   },
   "source": [
    "## Plot Two Categorical Variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bc5a09a7c9d504394785aa144a14547c31d3ab2f"
   },
   "outputs": [],
   "source": [
    "def plot_categoricals(x, y, data, annotate = True):\n",
    "    \"\"\"Plot counts of two categoricals.\n",
    "    Size is raw count for each grouping.\n",
    "    Percentages are for a given value of y.\"\"\"\n",
    "    \n",
    "    # Raw counts \n",
    "    raw_counts = pd.DataFrame(data.groupby(y)[x].value_counts(normalize = False))\n",
    "    raw_counts = raw_counts.rename(columns = {x: 'raw_count'})\n",
    "    \n",
    "    # Calculate counts for each group of x and y\n",
    "    counts = pd.DataFrame(data.groupby(y)[x].value_counts(normalize = True))\n",
    "    \n",
    "    # Rename the column and reset the index\n",
    "    counts = counts.rename(columns = {x: 'normalized_count'}).reset_index()\n",
    "    counts['percent'] = 100 * counts['normalized_count']\n",
    "    \n",
    "    # Add the raw count\n",
    "    counts['raw_count'] = list(raw_counts['raw_count'])\n",
    "    \n",
    "    plt.figure(figsize = (14, 10))\n",
    "    # Scatter plot sized by percent\n",
    "    plt.scatter(counts[x], counts[y], edgecolor = 'k', color = 'lightgreen',\n",
    "                s = 100 * np.sqrt(counts['raw_count']), marker = 'o',\n",
    "                alpha = 0.6, linewidth = 1.5)\n",
    "    \n",
    "    if annotate:\n",
    "        # Annotate the plot with text\n",
    "        for i, row in counts.iterrows():\n",
    "            # Put text with appropriate offsets\n",
    "            plt.annotate(xy = (row[x] - (1 / counts[x].nunique()), \n",
    "                               row[y] - (0.15 / counts[y].nunique())),\n",
    "                         color = 'navy',\n",
    "                         s = f\"{round(row['percent'], 1)}%\")\n",
    "        \n",
    "    # Set tick marks\n",
    "    plt.yticks(counts[y].unique())\n",
    "    plt.xticks(counts[x].unique())\n",
    "    \n",
    "    # Transform min and max to evenly space in square root domain\n",
    "    sqr_min = int(np.sqrt(raw_counts['raw_count'].min()))\n",
    "    sqr_max = int(np.sqrt(raw_counts['raw_count'].max()))\n",
    "    \n",
    "    # 5 sizes for legend\n",
    "    msizes = list(range(sqr_min, sqr_max,\n",
    "                        int(( sqr_max - sqr_min) / 5)))\n",
    "    markers = []\n",
    "    \n",
    "    # Markers for legend\n",
    "    for size in msizes:\n",
    "        markers.append(plt.scatter([], [], s = 100 * size, \n",
    "                                   label = f'{int(round(np.square(size) / 100) * 100)}', \n",
    "                                   color = 'lightgreen',\n",
    "                                   alpha = 0.6, edgecolor = 'k', linewidth = 1.5))\n",
    "        \n",
    "    # Legend and formatting\n",
    "    plt.legend(handles = markers, title = 'Counts',\n",
    "               labelspacing = 3, handletextpad = 2,\n",
    "               fontsize = 16,\n",
    "               loc = (1.10, 0.19))\n",
    "    \n",
    "    plt.annotate(f'* Size represents raw count while % is for a given y value.',\n",
    "                 xy = (0, 1), xycoords = 'figure points', size = 10)\n",
    "    \n",
    "    # Adjust axes limits\n",
    "    plt.xlim((counts[x].min() - (6 / counts[x].nunique()), \n",
    "              counts[x].max() + (6 / counts[x].nunique())))\n",
    "    plt.ylim((counts[y].min() - (4 / counts[y].nunique()), \n",
    "              counts[y].max() + (4 / counts[y].nunique())))\n",
    "    plt.grid(None)\n",
    "    plt.xlabel(f\"{x}\"); plt.ylabel(f\"{y}\"); plt.title(f\"{y} vs {x}\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a4dd0f5cb0567694952aeb24995cdf5d0f0a0cd3"
   },
   "outputs": [],
   "source": [
    "plot_categoricals('rez_esc', 'Target', data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f25ffc2c2a6c8a0c37217dacdccee3f3f7f5313f"
   },
   "source": [
    "The size of the markers represents the raw count. To read the plot, choose a given y-value and then read across the row. For example, with a poverty level of 1, 93% of individuals have no years behind with a total count of around 800 individuals and about 0.4% of individuals are 5 years behind with about 50 total individuals in this category. This plot attempts to show both the overall counts and the within category proportion; it's not perfect , but I gave it a shot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0bebbe6d0673a374c7d7c0dcb029c4dd92687ba5"
   },
   "outputs": [],
   "source": [
    "plot_categoricals('escolari', 'Target', data, annotate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6c3444e8f8baea05b29fe2490ee09bd8b255512f"
   },
   "source": [
    "The remaining missing values in each column will be filled in, a process known as `Imputation`. There are several types of imputation commonly used, and one of the simplest and most effective methods is to fill in the missing values with the `median` of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aab147c98444a34aee0259bcd505d878db00c86f"
   },
   "outputs": [],
   "source": [
    "plot_value_counts(data[(data['rez_esc-missing'] == 1)], \n",
    "                  'Target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5ac0324614abd5ee24f5f32277ca08b9bcbd72e6"
   },
   "source": [
    "The distribution here seems to match that for all the data at large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d6e74fff4f86c84e961debf2cce2cf54c2c7e87b"
   },
   "outputs": [],
   "source": [
    "plot_value_counts(data[(data['v2a1-missing'] == 1)], \n",
    "                  'Target')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a1b4f1d3869a4a216aaf1dee9c949d8b317c11f1"
   },
   "source": [
    "This looks like it could be an indicator of more poverty given the higher prevalence of 2: moderate poverty. \n",
    "\n",
    "__This represents an important point__: sometimes the missing information is just as important as the information you are given. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "989a4051c0166d6d910ff526fc30f50a3b58f969"
   },
   "source": [
    "# Feature Engineering\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "81fa4da4a8733fdfdd756e0a946ffb8a799fc51c"
   },
   "source": [
    "## Column Definitions\n",
    "\n",
    "Sometimes in data science we have to get our hands dirty digging through the data or do tedious tasks that take a lot of time. This is that part of the analysis: we have to define the columns that are at an individual level and at a household level using the [data decsriptions](https://www.kaggle.com/c/costa-rican-household-poverty-prediction/data). There is simply no other way to identify which variables at are the household level than to go through the variables themselves in the data description. Except, I've already done this for you, so all you have to do is copy and paste!\n",
    "\n",
    "We'll define different variables because we need to treat some of them in a different manner. Once we have the variables defined on each level, we can work to start aggregating them as needed.\n",
    "\n",
    "The process is as follows\n",
    "\n",
    "1. Break variables into household level and invididual level\n",
    "2. Find suitable aggregations for the individual level data\n",
    "    * Ordinal variables can use statistical aggregations\n",
    "    * Boolean variables can also be aggregated but with fewer stats\n",
    "3. Join the individual aggregations to the household level data\n",
    "\n",
    "### Define Variable Categories\n",
    "\n",
    "There are several different categories of variables:\n",
    "\n",
    "1. Individual Variables: these are characteristics of each individual rather than the household\n",
    "    * Boolean: Yes or No (0 or 1)\n",
    "    * Ordered Discrete: Integers with an ordering\n",
    "2. Household variables\n",
    "    * Boolean: Yes or No\n",
    "    * Ordered Discrete: Integers with an ordering\n",
    "    * Continuous numeric\n",
    "3. Squared Variables: derived from squaring variables in the data\n",
    "4. Id variables: identifies the data and should not be used as features\n",
    "\n",
    "Below we manually define the variables in each category. This is a little tedious, but also necessary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9e5a3b03ca064e4dc055e785730d1a43af037277"
   },
   "outputs": [],
   "source": [
    "id_ = ['Id', 'idhogar', 'Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dc6f4e6c5f343e917d65a1d8e6c1043236bcdce1"
   },
   "outputs": [],
   "source": [
    "ind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n",
    "            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n",
    "            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n",
    "            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n",
    "            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n",
    "            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n",
    "            'instlevel9', 'mobilephone', 'rez_esc-missing']\n",
    "\n",
    "ind_ordered = ['rez_esc', 'escolari', 'age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b382ddbea1cd6ff25b71f13b9263a6565ebadd57"
   },
   "outputs": [],
   "source": [
    "hh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n",
    "           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n",
    "           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n",
    "           'pisonatur', 'pisonotiene', 'pisomadera',\n",
    "           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n",
    "           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n",
    "            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n",
    "           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n",
    "           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n",
    "           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n",
    "           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n",
    "           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n",
    "           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n",
    "           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n",
    "           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2', 'v2a1-missing']\n",
    "\n",
    "hh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n",
    "              'r4t3', 'v18q1', 'tamhog','tamviv','hhsize','hogar_nin',\n",
    "              'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', 'qmobilephone']\n",
    "\n",
    "hh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2771c641c24bb3ed21eb655320a4ad85066a2cd6"
   },
   "outputs": [],
   "source": [
    "sqr_ = ['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', \n",
    "        'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0f6effa461740a3436b7af6c3e866014fe7742f6"
   },
   "source": [
    "Let's make sure we covered all of the variables and didn't repeat any. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8aa08338c60ae323e4e4d6654f1c4380befd5c76"
   },
   "outputs": [],
   "source": [
    "x = ind_bool + ind_ordered + id_ + hh_bool + hh_ordered + hh_cont + sqr_\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "print('There are no repeats: ', np.all(np.array(list(Counter(x).values())) == 1))\n",
    "print('We covered every variable: ', len(x) == data.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "39d839a711f0783285ff0ca570366f045eb995f1"
   },
   "source": [
    "#### Squared Variables\n",
    "\n",
    "First, the easiest step: we'll remove all of the squared variables. Sometimes variables are squared or transformed as part of feature engineering because it can help linear models learn relationships that are non-linear. However, since we will be using more complex models, these squared features are redundant. They are highly correlated with the non-squared version, and hence can actually hurt our model by adding irrelevant information and also slowing down training.\n",
    "\n",
    "For an example, let's take a look at `SQBage` vs `age`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5c89fc990cf416705e577fda9c086900f2c3abef"
   },
   "outputs": [],
   "source": [
    "sns.lmplot('age', 'SQBage', data = data, fit_reg=False);\n",
    "plt.title('Squared Age versus Age');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "489d3c75283d701f3ea74bc14db815f212b546bb"
   },
   "source": [
    "These variables are highly correlated, and we don't need to keep both in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b258b0875757bc29016c020164b21021b63a7bcc"
   },
   "outputs": [],
   "source": [
    "# Remove squared variables\n",
    "data = data.drop(columns = sqr_)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "68b5104b73966a357e6c278fccb562f52ba29863"
   },
   "source": [
    "## Id Variables\n",
    "\n",
    "These are pretty simple: they will be kept as is in the data since we need them for identification.\n",
    "\n",
    "## Household Level Variables\n",
    "\n",
    "First let's subset to the heads of household and then to the household level variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f3c177da83980642540c763666f6859d59d506aa"
   },
   "outputs": [],
   "source": [
    "heads = data.loc[data['parentesco1'] == 1, :]\n",
    "heads = heads[id_ + hh_bool + hh_cont + hh_ordered]\n",
    "heads.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "730b9df91ea83a5fce8097745fbc66e768c5d321"
   },
   "source": [
    "For most of the household level variables, we can simply keep them as is: since we want to make predictions for each household, we use these variables as features. However, we can also remove some redundant variables and also add in some more features derived from existing data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ff1b80ef402231ab5191a5f65e2c47ae7bcaf696"
   },
   "source": [
    "### Redundant Household Variables\n",
    "\n",
    "Let's take a look at the correlations between all of the household variables. If there are any that are too highly correlated, then we might want to remove one of the pair of highly correlated variables.\n",
    "\n",
    "The following code identifies any variables with a greater than 0.95 absolute magnitude correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ff9ed5e6364dadd624f5717fbc2b3ee37195e6f3"
   },
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = heads.corr()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n",
    "\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1ae294fb76f5bcaaaf61de9cc0abffefaf881cdf"
   },
   "source": [
    "These show one out of each pair of correlated variables. To find the other pair, we can subset the `corr_matrix`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a4c8970c265d7eff827abac3356ddd1d12b5572a"
   },
   "outputs": [],
   "source": [
    "corr_matrix.loc[corr_matrix['tamhog'].abs() > 0.9, corr_matrix['tamhog'].abs() > 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ae006c90765293d8a09f4fea910420cf77dde8db"
   },
   "outputs": [],
   "source": [
    "sns.heatmap(corr_matrix.loc[corr_matrix['tamhog'].abs() > 0.9, corr_matrix['tamhog'].abs() > 0.9],\n",
    "            annot=True, cmap = plt.cm.autumn_r, fmt='.3f');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "76b4979c18c990a853e907de0d5e68982eaabdc5"
   },
   "source": [
    "There are several variables here having to do with the size of the house:\n",
    "\n",
    "* r4t3, Total persons in the household\n",
    "* tamhog, size of the household\n",
    "* tamviv, number of persons living in the household\n",
    "* hhsize, household size\n",
    "* hogar_total, # of total individuals in the household\n",
    "\n",
    "These variables are all highly correlated with one another. In fact, `hhsize` has a perfect correlation with `tamhog` and `hogar_total`. We will remove these two variables because the information is redundant. We can also remove `r4t3` because it has a near perfect correlation with `hhsize`.\n",
    "\n",
    "`tamviv` is not necessarily the same as `hhsize` because there might be family members that are not living in the household. Let's visualize this difference in a scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b3517e3d0f11f886d82f2fcc7b07c3b03e455c8d"
   },
   "outputs": [],
   "source": [
    "heads = heads.drop(columns = ['tamhog', 'hogar_total', 'r4t3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cd03278e9ae4244036f60a223f9fb7f16da18b02"
   },
   "outputs": [],
   "source": [
    "sns.lmplot('tamviv', 'hhsize', data, fit_reg=False, size = 8);\n",
    "plt.title('Household size vs number of persons living in the household');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "907c00eb54bf710b7edf1a2729e6afd8bf4c276c"
   },
   "source": [
    "We see for a number of cases, there are more people living in the household than there are in the family. This gives us a good idea for a new feature: __the difference between these two measurements!__\n",
    "\n",
    "Let's make this new feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2cbebfb6c3274e08bb76feb9bd311f79f1f4b2fd"
   },
   "outputs": [],
   "source": [
    "heads['hhsize-diff'] = heads['tamviv'] - heads['hhsize']\n",
    "plot_categoricals('hhsize-diff', 'Target', heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a46229591b686ffbc89d3c3e6308b4505bb2ce85"
   },
   "source": [
    "Even though most households do not have a difference, there are a few that have more people living in the household than are members of the household.\n",
    "\n",
    "Let's move on to the other redundant variables. First we can look at `coopele`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "27ce4ffc97c6a745a85545cc2bd9fb4a5f0d4071"
   },
   "outputs": [],
   "source": [
    "corr_matrix.loc[corr_matrix['coopele'].abs() > 0.9, corr_matrix['coopele'].abs() > 0.9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "562839a941bac3265a5d0d4ce9eccc12e416bf0e"
   },
   "source": [
    "These variables indicate where the electricity in the home is coming from. There are four options, and the families that don't have one of these two options either have no electricity (`noelec`) or get it from a private plant (`planpri`). \n",
    "\n",
    "#### Creating Ordinal Variable\n",
    "\n",
    "I'm going to compress these four variables into one by creating an ordinal variable. I'm going to choose the mapping myself, based on the data decriptions: \n",
    "\n",
    "    0: No electricity\n",
    "    1: Electricity from cooperative\n",
    "    2: Electricity from CNFL, ICA, ESPH/JASEC\n",
    "    3: Electricity from private plant\n",
    "\n",
    "An ordered variable has an inherent ordering, and for this we choose our own based on the domain knowledge. After we create this new ordered variable, we can drop the four others. There are several households that do not have a variable here, so we will use a `nan` (which will be filled in during imputation) and add a Boolean column indicating there was no measure for this variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a9bd357a09eeccc0f0c479651470331f47af2648"
   },
   "outputs": [],
   "source": [
    "elec = []\n",
    "\n",
    "# Assign values\n",
    "for i, row in heads.iterrows():\n",
    "    if row['noelec'] == 1:\n",
    "        elec.append(0)\n",
    "    elif row['coopele'] == 1:\n",
    "        elec.append(1)\n",
    "    elif row['public'] == 1:\n",
    "        elec.append(2)\n",
    "    elif row['planpri'] == 1:\n",
    "        elec.append(3)\n",
    "    else:\n",
    "        elec.append(np.nan)\n",
    "        \n",
    "# Record the new variable and missing flag\n",
    "heads['elec'] = elec\n",
    "heads['elec-missing'] = heads['elec'].isnull()\n",
    "\n",
    "# Remove the electricity columns\n",
    "# heads = heads.drop(columns = ['noelec', 'coopele', 'public', 'planpri'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5d97954ff9c3420f9b44a8865e05fc473a17a1a5"
   },
   "outputs": [],
   "source": [
    "plot_categoricals('elec', 'Target', heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0276d599b977349915d0ac88e02d25b3cffd5357"
   },
   "source": [
    "We can see that for every value of the Target, the most common source of electricity is from one of the listed providers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5a2f14086e364572f84a05d21daad869cabe5378"
   },
   "source": [
    "The final redundant column is `area2`. This means the house is in a rural zone, but it's redundant because we have a column indicating if the house is in a urban zone. Therefore, we can drop this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8417d12ca211bd0aab75ba94d0215161000ccf81"
   },
   "outputs": [],
   "source": [
    "heads = heads.drop(columns = 'area2')\n",
    "\n",
    "heads.groupby('area1')['Target'].value_counts(normalize = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4400e7ac30ef507308ea9c7a24581eff0ad570ef"
   },
   "source": [
    "It seems like households in an urban area (value of 1) are more likely to have lower poverty levels than households in a rural area (value of 0)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0e7aa155b0c310c6ec4ac25c88c90af5e007e247"
   },
   "source": [
    "### Creating Ordinal Variables\n",
    "\n",
    "For the walls, roof, and floor of the house, there are three columns each: the first indicating 'bad', the second 'regular', and the third 'good'. We could leave the variables as booleans, but to me it makes more sense to turn them into ordinal variables because there is an inherent order: `bad < regular < good`. To do this, we can simply find whichever column is non-zero for each household using `np.argmax`. \n",
    "\n",
    "Once we have created the ordinal variable, we are able to drop the original variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8c776cec90c7f4829eda309d7c925e6c751008ab"
   },
   "outputs": [],
   "source": [
    "# Wall ordinal variable\n",
    "heads['walls'] = np.argmax(np.array(heads[['epared1', 'epared2', 'epared3']]),\n",
    "                           axis = 1)\n",
    "\n",
    "# heads = heads.drop(columns = ['epared1', 'epared2', 'epared3'])\n",
    "plot_categoricals('walls', 'Target', heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5df42c066b57cac634891f08570f33811961c42a"
   },
   "outputs": [],
   "source": [
    "# Roof ordinal variable\n",
    "heads['roof'] = np.argmax(np.array(heads[['etecho1', 'etecho2', 'etecho3']]),\n",
    "                           axis = 1)\n",
    "heads = heads.drop(columns = ['etecho1', 'etecho2', 'etecho3'])\n",
    "\n",
    "# Floor ordinal variable\n",
    "heads['floor'] = np.argmax(np.array(heads[['eviv1', 'eviv2', 'eviv3']]),\n",
    "                           axis = 1)\n",
    "# heads = heads.drop(columns = ['eviv1', 'eviv2', 'eviv3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "747de840f4fe3374f8ef95637b9948a1be5a737b"
   },
   "source": [
    "## Feature Construction\n",
    "\n",
    "In addition to mapping variables to ordinal features, we can also create entirely new features from the existing data, known as feature construction. For example, we can add up the previous three features we just created to get an overall measure of the quality of the house's structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f698c5f8bb5dcff40f1711939b9cdefd060cf8b0"
   },
   "outputs": [],
   "source": [
    "# Create new feature\n",
    "heads['walls+roof+floor'] = heads['walls'] + heads['roof'] + heads['floor']\n",
    "\n",
    "plot_categoricals('walls+roof+floor', 'Target', heads, annotate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ce2862b49ece2347ee94cce38c8c6ba316dfeb1d"
   },
   "source": [
    "This new feature may be useful because it seems like a Target of 4 (the lowest poverty level) tends to have higher values of the 'house quality' variable. We can also look at this in a table to get the fine-grained details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a0e0570396accb529420c25eb42224363daedd54"
   },
   "outputs": [],
   "source": [
    "counts = pd.DataFrame(heads.groupby(['walls+roof+floor'])['Target'].value_counts(normalize = True)).rename(columns = {'Target': 'Normalized Count'}).reset_index()\n",
    "counts.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "038c8d7ad491a74016bd47cfdbb8ea568c318515"
   },
   "source": [
    "The next variable will be a `warning` about the quality of the house. It will be a negative value, with -1 point each for no toilet, electricity, floor, water service, and ceiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1c69b16c9508d5c453fb3582a0731bee60116938"
   },
   "outputs": [],
   "source": [
    "# No toilet, no electricity, no floor, no water service, no ceiling\n",
    "heads['warning'] = 1 * (heads['sanitario1'] + \n",
    "                         (heads['elec'] == 0) + \n",
    "                         heads['pisonotiene'] + \n",
    "                         heads['abastaguano'] + \n",
    "                         (heads['cielorazo'] == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "40acec7571f26575f2001c08675ce88206ffb014"
   },
   "source": [
    "We can keep using our `plot_categoricals` function to visualize these relationships, but `seaborn` also has a number of plotting options that can work with categoricals. One is the `violinplot` which shows the distribution of a variable on the y axis with the width of each plot showing the number of observations in that category. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "44e3169ba43c0a59828965e1a091abbacea15f72"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "sns.violinplot(x = 'warning', y = 'Target', data = heads);\n",
    "plt.title('Target vs Warning Variable');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c8861533da1cdf69b1a2f0d08808b5c4cffd99ee"
   },
   "outputs": [],
   "source": [
    "plot_categoricals('warning', 'Target', data = heads)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "15d1d11a7c70bbf2cedba08a6a8af28e8a699d88"
   },
   "source": [
    "The violinplot is not great here because it smooths out the categorical variable with the effect that it looks as if the Target can take on lesser and greater values than in reality. Nonetheless, we can see a high concentration of households that have no warning signs and have the lowest level of poverty. It looks as if this may be a useful feature, but we can't know for sure until we get to modeling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9884e666ffb55adacc01931675170f39ab0886cf"
   },
   "source": [
    "The final household feature we can make for now is a `bonus` where a family gets a point for having a refrigerator, computer, tablet, or television."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "edf1cf9043391a9af16c45e19e84bbc91dce052a"
   },
   "outputs": [],
   "source": [
    "# Owns a refrigerator, computer, tablet, and television\n",
    "heads['bonus'] = 1 * (heads['refrig'] + \n",
    "                      heads['computer'] + \n",
    "                      (heads['v18q1'] > 0) + \n",
    "                      heads['television'])\n",
    "\n",
    "sns.violinplot('bonus', 'Target', data = heads,\n",
    "                figsize = (10, 6));\n",
    "plt.title('Target vs Bonus Variable');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "97bfbf33dddaed18073ba92bda62305cc38a770f"
   },
   "source": [
    "## Per Capita Features\n",
    "\n",
    "Additional features we can make calculate the number of certain measurements for each person in the household."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cc7c411527e8fd9955e8ab83bcec4c78322c6355"
   },
   "outputs": [],
   "source": [
    "heads['phones-per-capita'] = heads['qmobilephone'] / heads['tamviv']\n",
    "heads['tablets-per-capita'] = heads['v18q1'] / heads['tamviv']\n",
    "heads['rooms-per-capita'] = heads['rooms'] / heads['tamviv']\n",
    "heads['rent-per-capita'] = heads['v2a1'] / heads['tamviv']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "615275fff580b041d45a9d358468f3fd8900169d"
   },
   "source": [
    "## Exploring Household Variables\n",
    "\n",
    "After going to all the trouble of getting our features in order, now we can take a look at them in relation to the Target. We've already done a little of this, but now we can try to quantify relationships.\n",
    "\n",
    "### Measuring Relationships\n",
    "\n",
    "There are many ways for measuring relationships between two variables. Here we will examine two of these:\n",
    "\n",
    "1. The Pearson Correlation: from -1 to 1 measuring the linear relationship between two variables\n",
    "2. The Spearman Correlation: from -1 to 1 measuring the monotonic relationship between two variables\n",
    "\n",
    "The Spearman correlation is 1 if as one variable increases, the other does as well, even if the relationship is not linear. On the other hand, the Pearson correlation can only be one if the increase is exactly linear. These are best illustrated by example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "37b0c3e16ea4e9055bca656ce590cc532a5c3d92"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6827b8e42f0462baed0d74409386e1ec255b0f46"
   },
   "outputs": [],
   "source": [
    "def plot_corrs(x, y):\n",
    "    \"\"\"Plot data and show the spearman and pearson correlation.\"\"\"\n",
    "    \n",
    "    # Calculate correlations\n",
    "    spr = spearmanr(x, y).correlation\n",
    "    pcr = np.corrcoef(x, y)[0, 1]\n",
    "    \n",
    "    # Scatter plot\n",
    "    data = pd.DataFrame({'x': x, 'y': y})\n",
    "    plt.figure( figsize = (6, 4))\n",
    "    sns.regplot('x', 'y', data = data, fit_reg = False);\n",
    "    plt.title(f'Spearman: {round(spr, 2)}; Pearson: {round(pcr, 2)}');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb3d0ab8df4bdfab62e817d55437ab0857690784"
   },
   "outputs": [],
   "source": [
    "x = np.array(range(100))\n",
    "y = x ** 2\n",
    "\n",
    "plot_corrs(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "dbe1eac29065813b705c37448f01e7ac660ff784"
   },
   "source": [
    "The Spearman correlation is often considered to be better for ordinal variables such as the Target or the years of education. Most relationshisp in the real world aren't linear, and although the Pearson correlation can be an approximation of how related two variables are, it's inexact and not the best method of comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9ff7af6f2f0c34d67db3e352df99066de8343091"
   },
   "outputs": [],
   "source": [
    "x = np.array([1, 1, 1, 2, 3, 3, 4, 4, 4, 5, 5, 6, 7, 8, 8, 9, 9, 9])\n",
    "y = np.array([1, 2, 1, 1, 1, 1, 2, 2, 2, 2, 1, 3, 3, 2, 4, 2, 2, 4])\n",
    "\n",
    "plot_corrs(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "391b0659f036f8a6509640eaf5c3a4ef6906566d"
   },
   "source": [
    "In most cases, the values are very similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d1a3e7c95f4d24c1253110c0965e1cc8fd43480e"
   },
   "outputs": [],
   "source": [
    "x = np.array(range(-19, 20))\n",
    "y = 2 * np.sin(x)\n",
    "\n",
    "plot_corrs(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e1074fba48fed2034ce1f176cf2b6652ef661348"
   },
   "source": [
    "First, we'll calculate the Pearson correlation of every variable with the Target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "41762a06dff718cdc452c4181df92f352e24b9df"
   },
   "outputs": [],
   "source": [
    "# Use only training data\n",
    "train_heads = heads.loc[heads['Target'].notnull(), :].copy()\n",
    "\n",
    "pcorrs = pd.DataFrame(train_heads.corr()['Target'].sort_values()).rename(columns = {'Target': 'pcorr'}).reset_index()\n",
    "pcorrs = pcorrs.rename(columns = {'index': 'feature'})\n",
    "\n",
    "print('Most negatively correlated variables:')\n",
    "print(pcorrs.head())\n",
    "\n",
    "print('\\nMost positively correlated variables:')\n",
    "print(pcorrs.dropna().tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b54fc898fde336625149bda09c5c6bfbfb82b033"
   },
   "source": [
    "For the negative correlations, as we increase the variable, the Target decreases indicating the poverty severity increases. Therefore, as the `warning` increases, the poverty level also increases which makes sense because this was meant to show potential bad signs about a house. The `hogar_nin` is the number of children 0 - 19 in the family which also makes sense: younger children can be financial source of stress on a family leading to higher levels of poverty. Or, families with lower socioeconomic status have more children in the hopes that one of them will be able to succeed. Whatever the explanation, there is a [real link between family size and poverty](https://www.adb.org/sites/default/files/publication/157217/adbi-rp68.pdf)\n",
    "\n",
    "On the other hand, for the positive correlations, a higher value means a higher value of Target indicating the poverty severity decreases. The most highly correlated household level variable is `meaneduc`, the average education level of the adults in the household. This relationship between education and poverty intuitively makes sense: [greater levels of education generally correlate with lower levels of poverty](https://www.childfund.org/poverty-and-education/). We don't necessarily know which causes which, but we do know these tend to move in the same direction.\n",
    "\n",
    "The general guidelines for correlation values are below, but these will change depending on who you ask ([source](http://www.statstutor.ac.uk/resources/uploaded/pearsons.pdf) for these):\n",
    "\n",
    "*  .00-.19 very weak\n",
    "*  .20-.39 weak\n",
    "*  .40-.59 moderate\n",
    "*  .60-.79 strong\n",
    "*  .80-1.0 very strong\n",
    "\n",
    "What these correlations show is that there are some weak relationships that hopefully our model will be able to use to learn a mapping from the features to the Target.\n",
    "\n",
    "Now we can move on to the Spearman correlation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "57ff58385cf395ad96537cf775bfa80a61a45979"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category = RuntimeWarning)\n",
    "\n",
    "feats = []\n",
    "scorr = []\n",
    "pvalues = []\n",
    "\n",
    "# Iterate through each column\n",
    "for c in heads:\n",
    "    # Only valid for numbers\n",
    "    if heads[c].dtype != 'object':\n",
    "        feats.append(c)\n",
    "        \n",
    "        # Calculate spearman correlation\n",
    "        scorr.append(spearmanr(train_heads[c], train_heads['Target']).correlation)\n",
    "        pvalues.append(spearmanr(train_heads[c], train_heads['Target']).pvalue)\n",
    "\n",
    "scorrs = pd.DataFrame({'feature': feats, 'scorr': scorr, 'pvalue': pvalues}).sort_values('scorr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f804082f69658b46ad0c9a134f84122856dd37df"
   },
   "source": [
    "The Spearman correlation coefficient calculation also comes with a `pvalue` indicating the significance level of the relationship. Any `pvalue` less than 0.05 is genearally regarded as significant, although since we are doing multiple comparisons, we want to divide the p-value by the number of comparisons, a process known as the Bonferroni correction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2aa018089c7e349611c3a0ddb7247d40de9a4351"
   },
   "outputs": [],
   "source": [
    "print('Most negative Spearman correlations:')\n",
    "print(scorrs.head())\n",
    "print('\\nMost positive Spearman correlations:')\n",
    "print(scorrs.dropna().tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "65312c90b8be134c59ad0c1b182305f0cc6738b2"
   },
   "source": [
    "For the most part, the two methods of calculating correlations are in agreement. Just out of curiousity, we can look for the values that are furthest apart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9cf2669563f444e88edb40a22e77ac3c0debef63"
   },
   "outputs": [],
   "source": [
    "corrs = pcorrs.merge(scorrs, on = 'feature')\n",
    "corrs['diff'] = corrs['pcorr'] - corrs['scorr']\n",
    "\n",
    "corrs.sort_values('diff').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e8246b539b238758630826413d7a191ddf30488b"
   },
   "outputs": [],
   "source": [
    "corrs.sort_values('diff').dropna().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5a5a5f7ce58dfbd7652c97a877a027a053b66e26"
   },
   "source": [
    "The largest discrepancy in the correlations is `dependency`. We can make a scatterplot of the `Target` versus the `dependency` to visualize the relationship. We'll add a little jitter to the plot because these are both discrete variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8361ec53ddf8642b79a3e206bb9fee47514792ae"
   },
   "outputs": [],
   "source": [
    "sns.lmplot('dependency', 'Target', fit_reg = True, data = train_heads, x_jitter=0.05, y_jitter=0.05);\n",
    "plt.title('Target vs Dependency');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1575020ee3771a475db3243ececb639e5f7ef8e7"
   },
   "source": [
    "It's hard to see the relationship, but it's slightly negative: as the `dependency` increases, the value of the `Target` decreases. This makes sense: the `dependency` is the number of dependent individuals divided by the number of non-dependents. As we increase this value, the poverty severty tends to increase: having more dependent family members (who usually are non-working) leads to higher levels of poverty because they must be supported by the non-dependent family members. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c45feb5e458547023a97560328141a5e41a22ea7"
   },
   "outputs": [],
   "source": [
    "sns.lmplot('rooms-per-capita', 'Target', fit_reg = True, data = train_heads, x_jitter=0.05, y_jitter=0.05);\n",
    "plt.title('Target vs Rooms Per Capita');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "46e99d738f30a21006a91209721f03b085fa1fa2"
   },
   "source": [
    "#### Correlation Heatmap \n",
    "\n",
    "One of my favorite plots is the correlation heatmap because it shows a ton of info in one image. For the heatmap, we'll pick 7 variables and show the correlations between themselves and with the target. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b3535cc3f355808f220d78862360e7aede64d92c"
   },
   "outputs": [],
   "source": [
    "variables = ['Target', 'dependency', 'warning', 'walls+roof+floor', 'meaneduc',\n",
    "             'floor', 'r4m1', 'overcrowding']\n",
    "\n",
    "# Calculate the correlations\n",
    "corr_mat = train_heads[variables].corr().round(2)\n",
    "\n",
    "# Draw a correlation heatmap\n",
    "plt.rcParams['font.size'] = 18\n",
    "plt.figure(figsize = (12, 12))\n",
    "sns.heatmap(corr_mat, vmin = -0.5, vmax = 0.8, center = 0, \n",
    "            cmap = plt.cm.RdYlGn_r, annot = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "05b815c1126418ac5821be1053e602d36c02e0d8"
   },
   "source": [
    "This plot shows us that there are a number of variables that have a weak correlation with the `Target`. There are also high correlations between some variables (such as `floor` and `walls+roof+floor`) which could pose an issue because of collinearity. \n",
    "\n",
    "### Features Plot\n",
    "\n",
    "For the final exploration of the household level data, we can make a plot of some of the most correlated variables with the Target. This shows scatterplots on the upper triangle, kernel density estimate (kde) plots on the diagonal, and 2D KDE plots on the lower triangle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "012c5afa076a04df5d833c725e2fdcd097b783a5"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Copy the data for plotting\n",
    "plot_data = train_heads[['Target', 'dependency', 'walls+roof+floor',\n",
    "                         'meaneduc', 'overcrowding']]\n",
    "\n",
    "# Create the pairgrid object\n",
    "grid = sns.PairGrid(data = plot_data, size = 4, diag_sharey=False,\n",
    "                    hue = 'Target', hue_order = [4, 3, 2, 1], \n",
    "                    vars = [x for x in list(plot_data.columns) if x != 'Target'])\n",
    "\n",
    "# Upper is a scatter plot\n",
    "grid.map_upper(plt.scatter, alpha = 0.8, s = 20)\n",
    "\n",
    "# Diagonal is a histogram\n",
    "grid.map_diag(sns.kdeplot)\n",
    "\n",
    "# Bottom is density plot\n",
    "grid.map_lower(sns.kdeplot, cmap = plt.cm.OrRd_r);\n",
    "grid = grid.add_legend()\n",
    "plt.suptitle('Feature Plots Colored By Target', size = 32, y = 1.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "058c36fdb6108c2a84045d4592ad8e53002a3b14"
   },
   "source": [
    "We'll leave the feature engineering of the household variables for now. Later, we can come back to this step if we are not pleased with the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "cb4e91d2ce09ff60e61ec1f3be16932ed7d0acd5"
   },
   "outputs": [],
   "source": [
    "household_feats = list(heads.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "de174018faf038f680d405baa68a2aba93ab1b77"
   },
   "source": [
    "# Individual Level Variables\n",
    "\n",
    "There are two types of individual level variables: Boolean (1 or 0 for True or False) and ordinal (discrete values with a meaningful ordering). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "88e7cc6cd8acab811702215a2dc2b08b8e203dd2"
   },
   "outputs": [],
   "source": [
    "ind = data[id_ + ind_bool + ind_ordered]\n",
    "ind.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "931587721d828d6a534d41f34f9c6a561a454cf4"
   },
   "source": [
    "## Redundant Individual Variables\n",
    "\n",
    "We can do the same process we did with the household level variables to identify any redundant individual variables. We'll focus on any variables that have an absolute magnitude of the correlation coefficient greater than 0.95."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7b8c7059c959d69372304663e1c4a8d839d27eac"
   },
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = ind.corr()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n",
    "\n",
    "to_drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b5feba5e8b3843a9e255497497184b5e8ab4754f"
   },
   "source": [
    "This is simply the opposite of male! We can remove the male flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e34832557ff343d282c11259eb1e1e4c477b0158"
   },
   "outputs": [],
   "source": [
    "ind = ind.drop(columns = 'male')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f6d5fcca07c194b82ba1ea89b8290bbd71ef632b"
   },
   "source": [
    "### Creating Ordinal Variables\n",
    "\n",
    "Much as we did with the household level data, we can map existing columns to an ordinal variable. Here we will focus on the `instlevel_` variables which indicate the amount of education an individual has from `instlevel1`: no level of education to `instlevel9`: postgraduate education. \n",
    "\n",
    "To create the ordinal variable, for each individual, we will simply find which column is non-zero. The education has an inherent ordering (higher is better) so this conversion to an ordinal variable makes sense in the problem context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "870c1271305145593a6bee0fec5d8dd64141f384"
   },
   "outputs": [],
   "source": [
    "ind[[c for c in ind if c.startswith('instl')]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f0cc456f9ec85144e32702a98f2c82871d72d268"
   },
   "outputs": [],
   "source": [
    "ind['inst'] = np.argmax(np.array(ind[[c for c in ind if c.startswith('instl')]]), axis = 1)\n",
    "\n",
    "plot_categoricals('inst', 'Target', ind, annotate = False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2005b023394839c61300ad2f131fd55e7065b36e"
   },
   "source": [
    "Higher levels of education seem to correspond to less extreme levels of poverty. We do need to keep in mind this is on an individual level though and we eventually will have to aggregate this data at the household level. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "36dea7591a21564e8762547d098ae7c4ca2010b0"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 8))\n",
    "sns.violinplot(x = 'Target', y = 'inst', data = ind);\n",
    "plt.title('Education Distribution by Target');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bf9faa15211503eba64468154970d620b8c0126a"
   },
   "outputs": [],
   "source": [
    "# Drop the education columns\n",
    "# ind = ind.drop(columns = [c for c in ind if c.startswith('instlevel')])\n",
    "ind.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "132986030dc283109318780a2f6330a75b72e3aa"
   },
   "source": [
    "### Feature Construction\n",
    "\n",
    "We can make a few features using the existing data. For example, we can divide the years of schooling by the age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f673326c23b7d9b0806924f05af22c0601a07798"
   },
   "outputs": [],
   "source": [
    "ind['escolari/age'] = ind['escolari'] / ind['age']\n",
    "\n",
    "plt.figure(figsize = (10, 8))\n",
    "sns.violinplot('Target', 'escolari/age', data = ind);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "35dddba8210e8fe298f21a61812a8492ea9697d4"
   },
   "source": [
    "We can also take our new variable, `inst`, and divide this by the age. The final variable we'll name `tech`: this represents the combination of tablet and mobile phones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eee1804b83ecf9ac83fb6aef014fe6ac5d3f93f8"
   },
   "outputs": [],
   "source": [
    "ind['inst/age'] = ind['inst'] / ind['age']\n",
    "ind['tech'] = ind['v18q'] + ind['mobilephone']\n",
    "ind['tech'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fbaef320687392da25e028e2021e5d5be574bb7e"
   },
   "source": [
    "## Feature Engineering through Aggregations\n",
    "\n",
    "In order to incorporate the individual data into the household data, we need to aggregate it for each household. The simplest way to do this is to `groupby` the family id `idhogar` and then `agg` the data. For the aggregations for ordered or continuous variables, we can use six, five of which are built in to pandas, and one of which we define ourselves `range_`. The boolean aggregations can be the same, but this will create many redundant columns which we will then need to drop. For this case, we'll use the same aggregations and then go back and drop the redundant columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f59c9612532cfc67c2551268370b662d03fe84ed"
   },
   "outputs": [],
   "source": [
    "# Define custom function\n",
    "range_ = lambda x: x.max() - x.min()\n",
    "range_.__name__ = 'range_'\n",
    "\n",
    "# Group and aggregate\n",
    "ind_agg = ind.drop(columns = 'Target').groupby('idhogar').agg(['min', 'max', 'sum', 'count', 'std', range_])\n",
    "ind_agg.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1eb6430fcba5d962ade9aa7e866bb6f16892057e"
   },
   "source": [
    "With just that one line, we go from 30 features to 180. Next we can rename the columns to make it easier to keep track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b444e21422941ca1c26b0b3404c3e2a16d5af991"
   },
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "new_col = []\n",
    "for c in ind_agg.columns.levels[0]:\n",
    "    for stat in ind_agg.columns.levels[1]:\n",
    "        new_col.append(f'{c}-{stat}')\n",
    "        \n",
    "ind_agg.columns = new_col\n",
    "ind_agg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f196a265f84ec22177da1822d0d732ff6f886d13"
   },
   "outputs": [],
   "source": [
    "ind_agg.iloc[:, [0, 1, 2, 3, 6, 7, 8, 9]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "aa8ae6e58944998218d3ddae84390577a9979104"
   },
   "source": [
    "### Feature Selection \n",
    "\n",
    "As a first round of feature selection, we can remove one out of every pair of variables with a correlation greater than 0.95. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6dd33798e4f99f82ac5c1ec659906bedeb12febf"
   },
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "corr_matrix = ind_agg.corr()\n",
    "\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "\n",
    "# Find index of feature columns with correlation greater than 0.95\n",
    "to_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n",
    "\n",
    "print(f'There are {len(to_drop)} correlated columns to remove.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d1f7bc5fc7635bfbee47507666fa844fbeef8c50"
   },
   "source": [
    "We'll drop the columns and then merge with the `heads` data to create a final dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bccbcdbad813f7c1596fd8209de3df46e18db199"
   },
   "outputs": [],
   "source": [
    "ind_agg = ind_agg.drop(columns = to_drop)\n",
    "ind_feats = list(ind_agg.columns)\n",
    "\n",
    "# Merge on the household id\n",
    "final = heads.merge(ind_agg, on = 'idhogar', how = 'left')\n",
    "\n",
    "print('Final features shape: ', final.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3e0b6e903c0780560d5e3b4283dcca8be3a79cf3"
   },
   "outputs": [],
   "source": [
    "final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8b31ac05762ca5c3a29bbe9739e47d688fd5fc35"
   },
   "source": [
    "### Final Data Exploration\n",
    "\n",
    "We'll do a little bit of exploration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "32accf5f15275477da10863997bac77b1bb6113e"
   },
   "outputs": [],
   "source": [
    "corrs = final.corr()['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2e75e94d93a61cefe0f4c9fa6ac26cb285bafc0e"
   },
   "outputs": [],
   "source": [
    "corrs.sort_values().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e6c8a457da110b83db2fe47cf5c631c9d3bcce58"
   },
   "outputs": [],
   "source": [
    "corrs.sort_values().dropna().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5f3a895a60ad479fc2665b7ce1e88ddabed1f4c6"
   },
   "source": [
    "We can see some of the variables that we made are highly correlated with the Target. Whether these variables are actually _useful_ will be determined in the modeling stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7b2ed3ed02f7921fab8a9b2b57d98bdaf6f3ff03"
   },
   "outputs": [],
   "source": [
    "plot_categoricals('escolari-max', 'Target', final, annotate=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d1c19def77a277533c2f663564eb99c1a29b314d"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "sns.violinplot(x = 'Target', y = 'escolari-max', data = final);\n",
    "plt.title('Max Schooling by Target');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2adb7da9565c8e85f2899c5a8992d2ce13617a35"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "sns.boxplot(x = 'Target', y = 'escolari-max', data = final);\n",
    "plt.title('Max Schooling by Target');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "83d009bf8a4f8f4cf18085524e4a5fdebc043286",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "sns.boxplot(x = 'Target', y = 'meaneduc', data = final);\n",
    "plt.xticks([0, 1, 2, 3], poverty_mapping.values())\n",
    "plt.title('Average Schooling by Target');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c3c0a518e219d7e2bafe0dba04c0fae4c0f4c7be"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "sns.boxplot(x = 'Target', y = 'overcrowding', data = final);\n",
    "plt.xticks([0, 1, 2, 3], poverty_mapping.values())\n",
    "plt.title('Overcrowding by Target');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "18187d9d6e401a27362372a3ec361a40bc157d25"
   },
   "source": [
    "One other feature that might be useful is the gender of the head of household. Since we aggregated the data, we'll have to go back to the individual level data and find the gender for the head of household."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "bc12c2723bb2867b7890bba79fe0be1887ba3a4f"
   },
   "outputs": [],
   "source": [
    "head_gender = ind.loc[ind['parentesco1'] == 1, ['idhogar', 'female']]\n",
    "final = final.merge(head_gender, on = 'idhogar', how = 'left').rename(columns = {'female': 'female-head'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1ba7d757545ead1b25ab27bd2bf149ac9819e598"
   },
   "outputs": [],
   "source": [
    "final.groupby('female-head')['Target'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b251e03cfb73b70ecb1563150ed6ce5a5318cdb3"
   },
   "source": [
    "It looks like households where the head is female are slightly more likely to have a severe level of poverty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "03b2c39589c3488a9f1241aed8760718fb9f3176"
   },
   "outputs": [],
   "source": [
    "sns.violinplot(x = 'female-head', y = 'Target', data = final);\n",
    "plt.title('Target by Female Head of Household');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "601b30f76124044c2949bb3e95d58067b3be2e00"
   },
   "source": [
    "We can also look at the difference in average education by whether or not the family has a female head of household."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e81f71c5576a83dc59b43d53fa79f1b422705e4b"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 8))\n",
    "sns.boxplot(x = 'Target', y = 'meaneduc', hue = 'female-head', data = final);\n",
    "plt.title('Average Education by Target and Female Head of Household', size = 16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "796eba22d6d1ba7cc68ba0ab300613934bbb44bf"
   },
   "source": [
    "It looks like at every value of the `Target`, households with female heads have higher levels of education. Yet, we saw that overall, households with female heads are more likely to have severe poverty. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4691aa23a0407a63dec6da789c6da64c9c0186ba"
   },
   "outputs": [],
   "source": [
    "final.groupby('female-head')['meaneduc'].agg(['mean', 'count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d0d75824293f57247e833e2c1386ff807d56585d"
   },
   "source": [
    "Overall, the average education of households with female heads is slightly higher than those with male heads. I'm not too sure what to make of this, but it seems right to me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a6f26316452144bfeff36474c36c32d1334668ba"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Custom scorer for cross validation\n",
    "scorer = make_scorer(f1_score, greater_is_better=True, average = 'macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8037345f25c39fdaa811adfd38f65714678359a2"
   },
   "outputs": [],
   "source": [
    "# Labels for training\n",
    "train_labels = np.array(list(final[final['Target'].notnull()]['Target'].astype(np.uint8)))\n",
    "\n",
    "# Extract the training data\n",
    "train_set = final[final['Target'].notnull()].drop(columns = ['Id', 'idhogar', 'Target'])\n",
    "test_set = final[final['Target'].isnull()].drop(columns = ['Id', 'idhogar', 'Target'])\n",
    "\n",
    "# Submission base which is used for making submissions to the competition\n",
    "submission_base = test[['Id', 'idhogar']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e4459fe1d50077ba09995fb01664ade706905d10"
   },
   "outputs": [],
   "source": [
    "features = list(train_set.columns)\n",
    "\n",
    "pipeline = Pipeline([('imputer', Imputer(strategy = 'median')), \n",
    "                      ('scaler', MinMaxScaler())])\n",
    "\n",
    "# Fit and transform training data\n",
    "train_set = pipeline.fit_transform(train_set)\n",
    "test_set = pipeline.transform(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d288a06b9b05d4868c3771e1e35c7658eb678e3f"
   },
   "source": [
    "The data has no missing values and is scaled between zero and one. This means it can be directly used in any Scikit-Learn model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b1b0fde920b9cb7caa2653e417b9cfb7cb3db075"
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, random_state=10, \n",
    "                               n_jobs = -1)\n",
    "# 10 fold cross validation\n",
    "cv_score = cross_val_score(model, train_set, train_labels, cv = 10, scoring = scorer)\n",
    "\n",
    "print(f'10 Fold Cross Validation F1 Score = {round(cv_score.mean(), 4)} with std = {round(cv_score.std(), 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b68367d25404464a1e62f49eab871f4fd97d75e8"
   },
   "source": [
    "That score is not great, but it will serve as a baseline and leaves us plenty of room to improve! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d93a45c46832d0b7804a112925d19bb5739e586b"
   },
   "source": [
    "## Feature Importances\n",
    "\n",
    "With a tree-based model, we can look at the feature importances which show a relative ranking of the usefulness of features in the model. These represent the sum of the reduction in impurity at nodes that used the variable for splitting, but we don't have to pay much attention to the absolute value. Instead we'll focus on relative scores.\n",
    "\n",
    "If we want to view the feature importances, we'll have to train a model on the whole training set. Cross validation does not return the feature importances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "76e8463384bd4b81f6d6ba8288b0684571bac380"
   },
   "outputs": [],
   "source": [
    "model.fit(train_set, train_labels)\n",
    "\n",
    "# Feature importances into a dataframe\n",
    "feature_importances = pd.DataFrame({'feature': features, 'importance': model.feature_importances_})\n",
    "feature_importances.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a474c17b1394ef6c008c6b9133fd40b4d8f04e98"
   },
   "source": [
    "Below is a short function we'll use to plot the feature importances. I use this function a lot and often copy and paste it between scripts. I hope the documentation makes sense! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d9912b9bdd0204aea423ec6e610f7788726ed038"
   },
   "outputs": [],
   "source": [
    "def plot_feature_importances(df, n = 10, threshold = None):\n",
    "    \"\"\"Plots n most important features. Also plots the cumulative importance if\n",
    "    threshold is specified and prints the number of features needed to reach threshold cumulative importance.\n",
    "    Intended for use with any tree-based feature importances. \n",
    "    \n",
    "    Args:\n",
    "        df (dataframe): Dataframe of feature importances. Columns must be \"feature\" and \"importance\".\n",
    "    \n",
    "        n (int): Number of most important features to plot. Default is 15.\n",
    "    \n",
    "        threshold (float): Threshold for cumulative importance plot. If not provided, no plot is made. Default is None.\n",
    "        \n",
    "    Returns:\n",
    "        df (dataframe): Dataframe ordered by feature importances with a normalized column (sums to 1) \n",
    "                        and a cumulative importance column\n",
    "    \n",
    "    Note:\n",
    "    \n",
    "        * Normalization in this case means sums to 1. \n",
    "        * Cumulative importance is calculated by summing features from most to least important\n",
    "        * A threshold of 0.9 will show the most important features needed to reach 90% of cumulative importance\n",
    "    \n",
    "    \"\"\"\n",
    "    plt.style.use('fivethirtyeight')\n",
    "    \n",
    "    # Sort features with most important at the head\n",
    "    df = df.sort_values('importance', ascending = False).reset_index(drop = True)\n",
    "    \n",
    "    # Normalize the feature importances to add up to one and calculate cumulative importance\n",
    "    df['importance_normalized'] = df['importance'] / df['importance'].sum()\n",
    "    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n",
    "    \n",
    "    plt.rcParams['font.size'] = 12\n",
    "    \n",
    "    # Bar plot of n most important features\n",
    "    df.loc[:n, :].plot.barh(y = 'importance_normalized', \n",
    "                            x = 'feature', color = 'darkgreen', \n",
    "                            edgecolor = 'k', figsize = (12, 8),\n",
    "                            legend = False, linewidth = 2)\n",
    "\n",
    "    plt.xlabel('Normalized Importance', size = 18); plt.ylabel(''); \n",
    "    plt.title(f'{n} Most Important Features', size = 18)\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    \n",
    "    if threshold:\n",
    "        # Cumulative importance plot\n",
    "        plt.figure(figsize = (8, 6))\n",
    "        plt.plot(list(range(len(df))), df['cumulative_importance'], 'b-')\n",
    "        plt.xlabel('Number of Features', size = 16); plt.ylabel('Cumulative Importance', size = 16); \n",
    "        plt.title('Cumulative Feature Importance', size = 18);\n",
    "        \n",
    "        # Number of features needed for threshold cumulative importance\n",
    "        # This is the index (will need to add 1 for the actual number)\n",
    "        importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n",
    "        \n",
    "        # Add vertical line to plot\n",
    "        plt.vlines(importance_index + 1, ymin = 0, ymax = 1.05, linestyles = '--', colors = 'red')\n",
    "        plt.show();\n",
    "        \n",
    "        print('{} features required for {:.0f}% of cumulative importance.'.format(importance_index + 1, \n",
    "                                                                                  100 * threshold))\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c0d04a9495e7be357599a1c91ea196908efd0624"
   },
   "outputs": [],
   "source": [
    "norm_fi = plot_feature_importances(feature_importances, threshold=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5403f4fdc5a5bd8192a2ec3a09f062d8573f3c1b"
   },
   "source": [
    "__Education reigns supreme!__ The most important variable is the average amount of education in the household, followed by the maximum education of anyone in the household.  I have a suspicion these variables are highly correlated (collinear) which means we may want to remove one of them from the data. The other most important features are a combination of variables we created and variables that were already present in the data. \n",
    "\n",
    "It's interesting that we only need 106 of the ~180 features to account for 90% of the importance. This tells us that we may be able to remove some of the features. However, feature importances don't tell us which direction of the feature is important (for example, we can't use these to tell whether more or less education leads to more severe poverty) they only tell us which features the model considered relevant. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "540771805485f245fa5a0a7d18f5fc35b7169e2a"
   },
   "outputs": [],
   "source": [
    "def kde_target(df, variable):\n",
    "    \"\"\"Plots the distribution of `variable` in `df` colored by the `Target` column\"\"\"\n",
    "    \n",
    "    colors = {1: 'red', 2: 'orange', 3: 'blue', 4: 'green'}\n",
    "\n",
    "    plt.figure(figsize = (12, 8))\n",
    "    \n",
    "    df = df[df['Target'].notnull()]\n",
    "    \n",
    "    for level in df['Target'].unique():\n",
    "        subset = df[df['Target'] == level].copy()\n",
    "        sns.kdeplot(subset[variable].dropna(), \n",
    "                    label = f'Poverty Level: {level}', \n",
    "                    color = colors[int(subset['Target'].unique())])\n",
    "\n",
    "    plt.xlabel(variable); plt.ylabel('Density');\n",
    "    plt.title('{} Distribution'.format(variable.capitalize()));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3db2aab481d2c7a641b722c87ef9ff9eded0d102"
   },
   "outputs": [],
   "source": [
    "kde_target(final, 'meaneduc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0b74cce70fba4268ca56a5202f879309045c9eb0"
   },
   "outputs": [],
   "source": [
    "kde_target(final, 'escolari/age-range_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fb3574b0f4d56a904729cc7c800e5619e3e89e8f"
   },
   "source": [
    "# Model Selection\n",
    "\n",
    "Now that we have a good set of features, it's time to get into the modeling. We already tried one basic model, the Random Forest Classifier which delivered a best macro F1 of 0.35. However, in machine learning, there is no way to know ahead of time which model will work best for a given dataset. The following plot shows that __there are some problems where even Gaussian Naive Bayes will outperform a gradient boosting machine__. This is from [an excellent paper by Randal Olson that discusses many points of machine learning](https://psb.stanford.edu/psb-online/proceedings/psb18/olson.pdf)\n",
    "\n",
    "![algorithm_comparison](https://raw.githubusercontent.com/WillKoehrsen/Machine-Learning-Projects/master/algorithm_comparison.png)\n",
    "\n",
    "What this plot tells us is that we have to try out a number of different models to see which is optimal. Most people eventually settle on the __gradient boosting machine__ and we will try that out, but for now we'll take a look at some of the other options.  There are literally dozens (maybe hundreds) of multi-class machine learning models if we look at the [Scikit-Learn documentation](http://scikit-learn.org/stable/modules/multiclass.html). We don't have to try them all, but we should sample from the options.\n",
    "\n",
    "What we want to do is write a function that can evaluate a model. This will be pretty simple since we already wrote most of the code. In addition to the Random Forest Classifier, we'll try eight other Scikit-Learn models. Luckily, this dataset is relatively small and we can rapidly iterate through the models. We will make a dataframe to hold the results and the function will add a row to the dataframe for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "78a7710f39bab271c4320f42b9bbc7e3f9a4bd2f"
   },
   "outputs": [],
   "source": [
    "# Model imports\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegressionCV, RidgeClassifierCV\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "665cd410e664a6092b756a888727f166fbb5a5ed"
   },
   "outputs": [],
   "source": [
    "import warnings \n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "# Filter out warnings from models\n",
    "warnings.filterwarnings('ignore', category = ConvergenceWarning)\n",
    "warnings.filterwarnings('ignore', category = DeprecationWarning)\n",
    "warnings.filterwarnings('ignore', category = UserWarning)\n",
    "\n",
    "# Dataframe to hold results\n",
    "model_results = pd.DataFrame(columns = ['model', 'cv_mean', 'cv_std'])\n",
    "\n",
    "def cv_model(train, train_labels, model, name, model_results=None):\n",
    "    \"\"\"Perform 10 fold cross validation of a model\"\"\"\n",
    "    \n",
    "    cv_scores = cross_val_score(model, train, train_labels, cv = 10, scoring=scorer, n_jobs = -1)\n",
    "    print(f'10 Fold CV Score: {round(cv_scores.mean(), 5)} with std: {round(cv_scores.std(), 5)}')\n",
    "    \n",
    "    if model_results is not None:\n",
    "        model_results = model_results.append(pd.DataFrame({'model': name, \n",
    "                                                           'cv_mean': cv_scores.mean(), \n",
    "                                                            'cv_std': cv_scores.std()},\n",
    "                                                           index = [0]),\n",
    "                                             ignore_index = True)\n",
    "\n",
    "        return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ab07945c331294a68d600de58e4ed9c2739473bf"
   },
   "outputs": [],
   "source": [
    "model_results = cv_model(train_set, train_labels, LinearSVC(), \n",
    "                         'LSVC', model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2ea1ec18bda63d33c6e6482de5361d1728f41bbd"
   },
   "source": [
    "That's one model to cross off the list (although we didn't perform hyperparameter tuning so the actual performance could possibly be improved)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e5c13ea9fb13134e3b25d59bedf5b9f974e1517c"
   },
   "outputs": [],
   "source": [
    "model_results = cv_model(train_set, train_labels, \n",
    "                         GaussianNB(), 'GNB', model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a73eca1b3b7be54e3d6bae663169e67854f91a06"
   },
   "source": [
    "That performance is very poor. I don't think we need to revisit the Gaussian Naive Bayes method (although there are problems on which it can outperform the Gradient Boosting Machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8b1dca57fa07e406f1fe81fa320b3b97ab078d8e"
   },
   "outputs": [],
   "source": [
    "model_results = cv_model(train_set, train_labels, \n",
    "                         MLPClassifier(hidden_layer_sizes=(32, 64, 128, 64, 32)),\n",
    "                         'MLP', model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "84fb16fdda905c410ef60cef95016c85306fe81c"
   },
   "source": [
    "The multi-layer perceptron (a deep neural network) has decent performance. This might be an option if we are able to hyperparameter tune the network. However, the limited amount of data could be an issue with a neural network as these generally require hundreds of thousands of examples to learn effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2310f6d79426e16c9e3af6f8d4bfe03aa847d1b7"
   },
   "outputs": [],
   "source": [
    "model_results = cv_model(train_set, train_labels, \n",
    "                          LinearDiscriminantAnalysis(), \n",
    "                          'LDA', model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "44862a70f227c4a2ec0b73431f0fdeeeebf09f0a"
   },
   "source": [
    "__If you run `LinearDiscriminantAnalysis` without filtering out the `UserWarning`s, you get many messages saying \"Variables are collinear.\"__ This might give us a hint that we want to remove some collinear features! We might want to try this model again after removing the collinear variables because the score is comparable to the random forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5c5c3d651e96cfe01af34f93e6faa501fa35a94f"
   },
   "outputs": [],
   "source": [
    "model_results = cv_model(train_set, train_labels, \n",
    "                         RidgeClassifierCV(), 'RIDGE', model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "94459bb16bb7ddf3f49ee1a547818dd52efbe764"
   },
   "source": [
    "The linear model (with ridge regularization) does surprisingly well. This might indicate that a simple model can go a long way in this problem (although we'll probably end up using a more powerful method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b10b93b0caed61b6b54df8acfdda3e144c6499cf"
   },
   "outputs": [],
   "source": [
    "for n in [5, 10, 20]:\n",
    "    print(f'\\nKNN with {n} neighbors\\n')\n",
    "    model_results = cv_model(train_set, train_labels, \n",
    "                             KNeighborsClassifier(n_neighbors = n),\n",
    "                             f'knn-{n}', model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "49639627d4e1513ca07395b9a471295afc0a03c6"
   },
   "source": [
    "As one more attempt, we'll consider the ExtraTreesClassifier, a variant on the random forest using ensembles of decision trees as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eeab91bfb4b953e0c7aa5112625b65268537c662"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "model_results = cv_model(train_set, train_labels, \n",
    "                         ExtraTreesClassifier(n_estimators = 100, random_state = 10),\n",
    "                         'EXT', model_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4f4a3dc32b62b3fcc625e54f8d92afd9f1de2e7d"
   },
   "source": [
    "## Comparing Model Performance\n",
    "\n",
    "With the modeling results in a dataframe, we can plot them to see which model does the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d8c2a9a2df39625e6b1066ca950e44b5fe68862e"
   },
   "outputs": [],
   "source": [
    "model_results = cv_model(train_set, train_labels,\n",
    "                          RandomForestClassifier(100, random_state=10),\n",
    "                              'RF', model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8cbd9d6729288337c5c3cace44af254ebd480fce"
   },
   "outputs": [],
   "source": [
    "model_results.set_index('model', inplace = True)\n",
    "model_results['cv_mean'].plot.bar(color = 'orange', figsize = (8, 6),\n",
    "                                  yerr = list(model_results['cv_std']),\n",
    "                                  edgecolor = 'k', linewidth = 2)\n",
    "plt.title('Model F1 Score Results');\n",
    "plt.ylabel('Mean F1 Score (with error bar)');\n",
    "model_results.reset_index(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "876ff3ad8202066da47d7aea5d1784a981c81edb"
   },
   "source": [
    "The most likely candidate seems to be the Random Forest because it does best right out of the box.  While we didn't tune any of the hyperparameters so the comparison between models is not perfect, these results reflect those of many other Kaggle competitiors finding that tree-based ensemble methods (including the Gradient Boosting Machine) perform very well on structured datasets. Hyperparameter performance does improve the performance of machine learning models, but we don't have time to try all possible combinations of settings for all models. The graph below ([from the paper by Randal Olson](https://psb.stanford.edu/psb-online/proceedings/psb18/olson.pdf)) shows the effect of hyperparameter tuning versus the default values in Scikit-Learn.\n",
    "\n",
    "![hyperparameter_improvement](https://raw.githubusercontent.com/WillKoehrsen/Machine-Learning-Projects/master/hyperparameter_improvement.png)\n",
    "\n",
    "In most cases the accuracy gain is less than 10% so the worst model is probably not suddenly going to become the best model through tuning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e9b271483c048ab27e584577a94f0dc4a2e47e6"
   },
   "source": [
    "For now we'll say the random forest does the best. Later we'll look at using the Gradient Boosting Machine, although not implemented in Scikit-Learn. Instead we'll be using the more powerful [LightGBM version](http://lightgbm.readthedocs.io/en/latest/). Now, let's turn to making a submission using the random forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "de34b3435f0ff50a2c437799867de0689fc91c7c"
   },
   "source": [
    "# Making a Submission\n",
    "\n",
    "In order to make a submission, we need the test data. Fortunately, we have the test data formatted in exactly the same manner as the train data. \n",
    "\n",
    "The format of a testing submission is shown below. Although we are making predictions for each household, we actually need one row per individual (identified by the `Id`) but only the prediction for the head of household is scored. \n",
    "\n",
    "```\n",
    "Id,Target\n",
    "ID_2f6873615,1\n",
    "ID_1c78846d2,2\n",
    "ID_e5442cf6a,3\n",
    "ID_a8db26a79,4\n",
    "ID_a62966799,4 \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3e38e6c120980df2d11f1fc5a2377b674b066481"
   },
   "source": [
    "The `submission_base` will have all the individuals in the test set since we have to have a \"prediction\" for each individual while the `test_ids` will only contain the `idhogar` from the heads of households. When predicting, we only predict for each household and then we merge the `predictions` dataframe with all of the individuals on the household id (`idhogar`). This will set the `Target` to the same value for everyone in a household. For the test households without a head of household, we can just set these predictions to 4 since they will not be scored. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "98fa2a883bf9365159dbbc51a15fb9c7ed1c1c10"
   },
   "outputs": [],
   "source": [
    "test_ids = list(final.loc[final['Target'].isnull(), 'idhogar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "444972fb20122f5a73f868789ecd170e2ffdced8"
   },
   "outputs": [],
   "source": [
    "def submit(model, train, train_labels, test, test_ids):\n",
    "    \"\"\"Train and test a model on the dataset\"\"\"\n",
    "    \n",
    "    # Train on the data\n",
    "    model.fit(train, train_labels)\n",
    "    predictions = model.predict(test)\n",
    "    predictions = pd.DataFrame({'idhogar': test_ids,\n",
    "                               'Target': predictions})\n",
    "\n",
    "     # Make a submission dataframe\n",
    "    submission = submission_base.merge(predictions, \n",
    "                                       on = 'idhogar',\n",
    "                                       how = 'left').drop(columns = ['idhogar'])\n",
    "    \n",
    "    # Fill in households missing a head\n",
    "    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n",
    "\n",
    "    return submission "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a0eeba88ccae80a22fad6d62ce924f6b517183d6"
   },
   "outputs": [],
   "source": [
    "rf_submission = submit(RandomForestClassifier(n_estimators = 100, \n",
    "                                              random_state=10, n_jobs = -1), \n",
    "                         train_set, train_labels, test_set, test_ids)\n",
    "\n",
    "rf_submission.to_csv('rf_submission.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "22bee5cc58b40acd2badce0579283671ef488ed6"
   },
   "source": [
    "# Upgrading Our Model: Gradient Boosting Machine\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "956832879bc14aef10d7f676e1148173d4e16a97"
   },
   "outputs": [],
   "source": [
    "def macro_f1_score(labels, predictions):\n",
    "    # Reshape the predictions as needed\n",
    "    predictions = predictions.reshape(len(np.unique(labels)), -1 ).argmax(axis = 0)\n",
    "    \n",
    "    metric_value = f1_score(labels, predictions, average = 'macro')\n",
    "    \n",
    "    # Return is name, value, is_higher_better\n",
    "    return 'macro_f1', metric_value, True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bdfde0db12b0e51f6e31d404126ab8036201f132"
   },
   "source": [
    "# Light Gradient Boosting Machine Implementation\n",
    "\n",
    "The function below implements training the gradient boosting machine with Stratified Kfold cross validation and early stopping to prevent overfitting to the training data (although this can still occur). The function performs training with cross validation and records the predictions in probability for each fold. To see how this works, we can return the predictions from each fold and then we'll return a submission to upload to the competition.\n",
    "\n",
    "Choosing hyperparameters for the Gradient Boosting Machine can be tough and generally is done through model optimization. In this notebook, we'll use a set of hyperparameters that I've found work well on previous problems (although they will not necessarily translate to this competition). \n",
    "\n",
    "We set the `n_estimators` to 10000 but we won't actually reach this number because we are using early stopping which will quit training estimators when the cross validation metric does not improve for `early_stopping_rounds`. There's a lot going on in this function, and read through it carefully to make sure you have it all! I've tried to make the comments and code straightforward. (The `display` is used to show custom information during training in combination with `%%capture` so we don't have to see all the LightGBM information during training)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "fac36cb103709ca76cf85e874bb2b1c7a378d9a1"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import lightgbm as lgb\n",
    "from IPython.display import display\n",
    "\n",
    "def model_gbm(features, labels, test_features, test_ids, \n",
    "              nfolds = 5, return_preds = False, hyp = None):\n",
    "    \"\"\"Model using the GBM and cross validation.\n",
    "       Trains with early stopping on each fold.\n",
    "       Hyperparameters probably need to be tuned.\"\"\"\n",
    "    \n",
    "    feature_names = list(features.columns)\n",
    "\n",
    "    # Option for user specified hyperparameters\n",
    "    if hyp is not None:\n",
    "        # Using early stopping so do not need number of esimators\n",
    "        if 'n_estimators' in hyp:\n",
    "            del hyp['n_estimators']\n",
    "        params = hyp\n",
    "    \n",
    "    else:\n",
    "        # Model hyperparameters\n",
    "        params = {'boosting_type': 'dart', \n",
    "                  'colsample_bytree': 0.88, \n",
    "                  'learning_rate': 0.028, \n",
    "                   'min_child_samples': 10, \n",
    "                   'num_leaves': 36, 'reg_alpha': 0.76, \n",
    "                   'reg_lambda': 0.43, \n",
    "                   'subsample_for_bin': 40000, \n",
    "                   'subsample': 0.54, \n",
    "                   'class_weight': 'balanced'}\n",
    "    \n",
    "    # Build the model\n",
    "    model = lgb.LGBMClassifier(**params, objective = 'multiclass', \n",
    "                               n_jobs = -1, n_estimators = 10000,\n",
    "                               random_state = 10)\n",
    "    \n",
    "    # Using stratified kfold cross validation\n",
    "    strkfold = StratifiedKFold(n_splits = nfolds, shuffle = True)\n",
    "    \n",
    "    # Hold all the predictions from each fold\n",
    "    predictions = pd.DataFrame()\n",
    "    importances = np.zeros(len(feature_names))\n",
    "    \n",
    "    # Convert to arrays for indexing\n",
    "    features = np.array(features)\n",
    "    test_features = np.array(test_features)\n",
    "    labels = np.array(labels).reshape((-1 ))\n",
    "    \n",
    "    valid_scores = []\n",
    "    \n",
    "    # Iterate through the folds\n",
    "    for i, (train_indices, valid_indices) in enumerate(strkfold.split(features, labels)):\n",
    "        \n",
    "        # Dataframe for fold predictions\n",
    "        fold_predictions = pd.DataFrame()\n",
    "        \n",
    "        # Training and validation data\n",
    "        X_train = features[train_indices]\n",
    "        X_valid = features[valid_indices]\n",
    "        y_train = labels[train_indices]\n",
    "        y_valid = labels[valid_indices]\n",
    "        \n",
    "        # Train with early stopping\n",
    "        model.fit(X_train, y_train, early_stopping_rounds = 100, \n",
    "                  eval_metric = macro_f1_score,\n",
    "                  eval_set = [(X_train, y_train), (X_valid, y_valid)],\n",
    "                  eval_names = ['train', 'valid'],\n",
    "                  verbose = 200)\n",
    "        \n",
    "        # Record the validation fold score\n",
    "        valid_scores.append(model.best_score_['valid']['macro_f1'])\n",
    "        \n",
    "        # Make predictions from the fold as probabilities\n",
    "        fold_probabilitites = model.predict_proba(test_features)\n",
    "        \n",
    "        # Record each prediction for each class as a separate column\n",
    "        for j in range(4):\n",
    "            fold_predictions[(j + 1)] = fold_probabilitites[:, j]\n",
    "            \n",
    "        # Add needed information for predictions \n",
    "        fold_predictions['idhogar'] = test_ids\n",
    "        fold_predictions['fold'] = (i+1)\n",
    "        \n",
    "        # Add the predictions as new rows to the existing predictions\n",
    "        predictions = predictions.append(fold_predictions)\n",
    "        \n",
    "        # Feature importances\n",
    "        importances += model.feature_importances_ / nfolds   \n",
    "        \n",
    "        # Display fold information\n",
    "        display(f'Fold {i + 1}, Validation Score: {round(valid_scores[i], 5)}, Estimators Trained: {model.best_iteration_}')\n",
    "\n",
    "    # Feature importances dataframe\n",
    "    feature_importances = pd.DataFrame({'feature': feature_names,\n",
    "                                        'importance': importances})\n",
    "    \n",
    "    valid_scores = np.array(valid_scores)\n",
    "    display(f'{nfolds} cross validation score: {round(valid_scores.mean(), 5)} with std: {round(valid_scores.std(), 5)}.')\n",
    "    \n",
    "    # If we want to examine predictions don't average over folds\n",
    "    if return_preds:\n",
    "        predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n",
    "        predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n",
    "        return predictions, feature_importances\n",
    "    \n",
    "    # Average the predictions over folds\n",
    "    predictions = predictions.groupby('idhogar', as_index = False).mean()\n",
    "    \n",
    "    # Find the class and associated probability\n",
    "    predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n",
    "    predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n",
    "    predictions = predictions.drop(columns = ['fold'])\n",
    "    \n",
    "    # Merge with the base to have one prediction for each individual\n",
    "    submission = submission_base.merge(predictions[['idhogar', 'Target']], on = 'idhogar', how = 'left').drop(columns = ['idhogar'])\n",
    "        \n",
    "    # Fill in the individuals that do not have a head of household with 4 since these will not be scored\n",
    "    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n",
    "    \n",
    "    # return the submission and feature importances along with validation scores\n",
    "    return submission, feature_importances, valid_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "31a574ac08cd9d10be79d86d6ef40fd1b06beb8e"
   },
   "source": [
    "### Cross Validation with Early Stopping Notes\n",
    "\n",
    "Cross validation with early stopping is one of the most effective methods for preventing overfitting on the training set because it prevents us from continuing to add model complexity once it is clear that validation scores are not improving. Repeating this process across multiple folds helps to reduce the bias that comes from using a single fold. Early stopping also lets us train the model much quicker. Overall, __early stopping with cross validation__ is the best method to select the number of estimators in the Gradient Boosting Machine and should be our default technique when we desig an implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "99c07a92300f43c7df45e7eed1e175b1adc74304"
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "predictions, gbm_fi = model_gbm(train_set, train_labels, test_set, test_ids, return_preds=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e0ed4f9d0ce7c6c3a5ced03017702e706907b02b"
   },
   "source": [
    "The power of the Gradient Boosting Machine can be seen here! The cross validation score blows away anything we've done previously. \n",
    "\n",
    "Let's take a look at the predictions to understand what is going on with the predictions in each fold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "583d73e53388439db5fdf62f750c045c8338b069"
   },
   "outputs": [],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cfaa253938c54c139df95ddba27ce260f76c829f"
   },
   "source": [
    "For each fold, the `1, 2, 3, 4` columns represent the probability for each `Target`. The `Target` is the maximum of these with the `confidence` the probability. We have the predictions for all 5 folds, so we can plot the confidence in each `Target` for the different folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0691d5a568b072f7818e73e7924f959eab8167f2"
   },
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = 18\n",
    "\n",
    "# Kdeplot\n",
    "g = sns.FacetGrid(predictions, row = 'fold', hue = 'Target', size = 3, aspect = 4)\n",
    "g.map(sns.kdeplot, 'confidence');\n",
    "g.add_legend();\n",
    "\n",
    "plt.suptitle('Distribution of Confidence by Fold and Target', y = 1.05);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "191a6a91c7e030d7c2314b8444b7010391f0ed5e"
   },
   "source": [
    "What we see here is that the confidence for each class if relatively low. It does appear that the model has greater confidence in `Target=4` predictions which makes sense because of the _class imbalance and the high prevalence of this label._ \n",
    "\n",
    "Another way to look at the information is as a `violinplot`. This shows the same information, with the number of observations related to the width of the plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c87e224213269b6ff51c8321389810f33820cec3"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (24, 12))\n",
    "sns.violinplot(x = 'Target', y = 'confidence', hue = 'fold', data = predictions);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "90c5e31ebd374d163030bc331ca52f60fe076891"
   },
   "source": [
    "Overall, these results show the issue with imbalanced class problems: our model cannot distinguish very well between the classes that are underrepresented. Later we'll look at predictions themselves and see where our model is \"confused\". For now, we can generate a submission file and submit it to the competition.\n",
    "\n",
    "When we actually make predictions for each household, we average the predictions from each of the folds. Therefore, we are essentially using multiple models since each one is trained on a slightly different fold of the data. The gradient boosting machine is already an ensemble machine learning model, and now we are using it almost as a meta-ensemble by averaging predictions from several gbms. \n",
    "\n",
    "This process is shown in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1c2f0e0a3f821c665dc58dd04ee21446483f5954"
   },
   "outputs": [],
   "source": [
    "# Average the predictions over folds\n",
    "predictions = predictions.groupby('idhogar', as_index = False).mean()\n",
    "\n",
    "# Find the class and associated probability\n",
    "predictions['Target'] = predictions[[1, 2, 3, 4]].idxmax(axis = 1)\n",
    "predictions['confidence'] = predictions[[1, 2, 3, 4]].max(axis = 1)\n",
    "predictions = predictions.drop(columns = ['fold'])\n",
    "\n",
    "# Plot the confidence by each target\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.boxplot(x = 'Target', y = 'confidence', data = predictions);\n",
    "plt.title('Confidence by Target');\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "sns.violinplot(x = 'Target', y = 'confidence', data = predictions);\n",
    "plt.title('Confidence by Target');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "12baeff0594ef621a9770ff02eb62bcff1e5e979"
   },
   "source": [
    "We can have the function instead return the actual submission file. This takes the average predictions across the five folds, in effectm combining 5 different models, each one trained on a slghtly different subset of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a772baa2d12f38ee099e34cc8c65b073aac504fe"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "submission, gbm_fi, valid_scores = model_gbm(train_set, train_labels, \n",
    "                                             test_set, test_ids, return_preds=False)\n",
    "\n",
    "submission.to_csv('gbm_baseline.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ea54214d3a7c1a0a75e1260f44dcc2c916ed2a14"
   },
   "outputs": [],
   "source": [
    "_ = plot_feature_importances(gbm_fi, threshold=0.95)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
